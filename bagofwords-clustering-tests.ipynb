{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.57735026919\n  (0, 1)\t0.57735026919\n  (0, 0)\t0.57735026919\n  (1, 2)\t0.666666666667\n  (1, 1)\t0.666666666667\n  (1, 0)\t0.333333333333\n  (2, 2)\t0.333333333333\n  (2, 1)\t0.666666666667\n  (2, 0)\t0.666666666667\n  (3, 2)\t0.267261241912\n  (3, 1)\t0.801783725737\n  (3, 0)\t0.534522483825\n  (4, 2)\t0.301511344578\n  (4, 1)\t0.301511344578\n  (4, 0)\t0.904534033733\n  (5, 2)\t0.267261241912\n  (5, 1)\t0.534522483825\n  (5, 0)\t0.801783725737\n  (6, 2)\t0.229415733871\n  (6, 1)\t0.688247201612\n  (6, 0)\t0.688247201612\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "data = [\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 2],\n",
    "    [2, 2, 1],\n",
    "    [2, 3, 1],\n",
    "    [3, 1, 1],\n",
    "    [3, 2, 1],\n",
    "    [3, 3, 1]\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "t = TfidfTransformer()\n",
    "d = t.fit_transform(data)\n",
    "print(d)\n",
    "\n",
    "\n",
    "# spam_corpus = map(string., [ \"buy viagra\", \"buy antibody\" ])\n",
    "# spam_corpus = [\"buy\", \"cra\", \"buy\", \"antibody\"]\n",
    "# unique_words = set([word for doc in spam_corpus for word in doc])\n",
    "# word_counts = [(word, map(lambda doc: doc.count(word), spam_corpus)) for word in unique_words]\n",
    "# spam_bag_of_words = pd.DataFrame(dict(word_counts))\n",
    "# print(spam_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   0.,  33.],\n       [  1.,   0.,   0.,   3.],\n       [  0.,   1.,   0.,  12.],\n       [  0.,   1.,   0.,   1.],\n       [  0.,   0.,   1.,  18.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements = [\n",
    "   {'city': 'Dubai', 'temperature': 33.},\n",
    "   {'city': 'Dubai', 'temperature': 3.},\n",
    "   {'city': 'London', 'temperature': 12.},\n",
    "   {'city': 'London', 'temperature': 1.},\n",
    "   {'city': 'San Fransisco', 'temperature': 18.}]\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "vec.fit_transform(measurements).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.,  20.,   0.],\n       [  0.,  10.,  10.],\n       [ 10.,  10.,  10.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "bag = [\n",
    "    [1, 10, 10],\n",
    "    [1, 20, 20],\n",
    "    [2, 20, 10],\n",
    "    [2, 30, 10],\n",
    "    [3, 10, 10],\n",
    "    [3, 20, 10],\n",
    "    [3, 30, 10]\n",
    "]\n",
    "bagarray = np.asarray(bag)\n",
    "# print(set(bagarray[...,0]))\n",
    "bagdicts = []\n",
    "for i in set(bagarray[...,0]):\n",
    "    bagdicts.append({})\n",
    "for w in bag:\n",
    "    docid = w[0]\n",
    "    wordid = w[1]\n",
    "    wordcount = w[2]\n",
    "    dict = bagdicts[docid-1]\n",
    "    dict[wordid] = wordcount\n",
    "# print(bagdicts)\n",
    "    \n",
    "# D2 = [\n",
    "#     {1: 1, 2: 2},\n",
    "#     {2: 1, 3: 1},\n",
    "#     {1: 1, 2: 1, 3: 1}\n",
    "# ]\n",
    "X = v.fit_transform(bagdicts)\n",
    "X\n",
    "# v.inverse_transform(X) == \\\n",
    "#     [{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]\n",
    "# True\n",
    "# >>> v.transform({'foo': 4, 'unseen_feature': 3})\n",
    "# array([[ 0.,  0.,  4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430 files\n6906 terms in vocabulary\n137 True\n"
     ]
    }
   ],
   "source": [
    "vocabfile = \"vocab.kos.txt\"\n",
    "with open(vocabfile, 'r') as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]\n",
    "# print(vocab[0:10])\n",
    "\n",
    "line = \"1 61 2\\n\"\n",
    "line.split()\n",
    "\n",
    "docwordfile = \"docword.kos.txt\"\n",
    "# bagdicts = [{} for i in range(0, len(vocab))]\n",
    "bagdicts = []\n",
    "count = 0\n",
    "with open(docwordfile, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        w = line.strip().split()\n",
    "        if len(w) == 3:\n",
    "            docid = int(w[0]) - 1\n",
    "            wordid = int(w[1]) - 1\n",
    "            wordcount = int(w[2])\n",
    "            if len(bagdicts) <= docid:\n",
    "                bagdicts.append({})  # fuuuu\n",
    "            dict = bagdicts[docid]\n",
    "            # dict[vocab[wordid]] = wordcount\n",
    "            dict[wordid] = wordcount\n",
    "            count += 1\n",
    "# print(count)\n",
    "print(len(bagdicts), \"files\")\n",
    "print(len(vocab), \"terms in vocabulary\")\n",
    "\n",
    "sum = 0\n",
    "for w in bagdicts[0]:\n",
    "    sum += bagdicts[0].get(w)\n",
    "print(sum, sum == 137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.0 True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "counts = v.fit_transform(bagdicts)\n",
    "sum = np.sum(counts[0])\n",
    "print(sum, sum == 137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m samples: 3430, n features: 6906\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "print(\"m samples: %d, n features: %d\" % tfidf.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document clustering with KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n    verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.066s\n\nTop terms per cluster:\nCluster 0: november account electoral governor poll senate polls house republicans vote\nCluster 1: bush kerry iraq president war campaign general house poll administration\nCluster 2: dean edwards clark kerry lieberman primary gephardt poll iowa democratic\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "clusters = 3\n",
    "km = KMeans(n_clusters=clusters,\n",
    "            init='k-means++',  # or 'random' (random centroids) \n",
    "            n_init=10,  # number of time the k-means algorithm will be run with different centroid seeds.    \n",
    "            max_iter=300\n",
    "            )\n",
    "\n",
    "print(\"Document clustering with %s\" % km)\n",
    "start = time()\n",
    "km.fit(tfidf)\n",
    "print(\"done in %0.3fs\" % (time() - start))\n",
    "print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "# terms = vectorizer.get_feature_names()\n",
    "for i in range(clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % vocab[ind], end='')\n",
    "    print()\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "# print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "# print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "corpus = [\n",
    "'This is the first document.',\n",
    "'This is the second second document.',\n",
    "'And the third one.',\n",
    "'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse\n",
    "document-frequency. This is a common term weighting scheme in information\n",
    "retrieval, that has also found good use in document classification.\n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
    "token in a given document is to scale down the impact of tokens that occur\n",
    "very frequently in a given corpus and that are hence empirically less\n",
    "informative than features that occur in a small fraction of the training\n",
    "corpus.\n",
    "\n",
    "The actual formula used for tf-idf is tf * (idf + 1) = tf + tf * idf,\n",
    "instead of tf * idf. The effect of this is that terms with zero idf, i.e.\n",
    "that occur in all documents of a training set, will not be entirely\n",
    "ignored. The formulas used to compute tf and idf depend on parameter\n",
    "settings that correspond to the SMART notation used in IR, as follows:\n",
    "\n",
    "Tf is \"n\" (natural) by default, \"l\" (logarithmic) when sublinear_tf=True.\n",
    "Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
    "Normalization is \"c\" (cosine) when norm='l2', \"n\" (none) when norm=None.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "norm : 'l1', 'l2' or None, optional\n",
    "    Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "use_idf : boolean, default=True\n",
    "    Enable inverse-document-frequency reweighting.\n",
    "\n",
    "smooth_idf : boolean, default=True\n",
    "    Smooth idf weights by adding one to document frequencies, as if an\n",
    "    extra document was seen containing every term in the collection\n",
    "    exactly once. Prevents zero divisions.\n",
    "\n",
    "sublinear_tf : boolean, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [Yates2011] `R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n",
    "               Information Retrieval. Addison Wesley, pp. 68-74.`\n",
    "\n",
    ".. [MRS2008] `C.D. Manning, P. Raghavan and H. Schuetze  (2008).\n",
    "               Introduction to Information Retrieval. Cambridge University\n",
    "               Press, pp. 118-120.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85151335  0.          0.52433293]\n [ 1.          0.          0.        ]\n [ 1.          0.          0.        ]\n [ 1.          0.          0.        ]\n [ 0.55422893  0.83236428  0.        ]\n [ 0.63035731  0.          0.77630514]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  2.25276297,  1.84729786])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "counts = [\n",
    "    [3, 0, 1], # doc1?\n",
    "    [2, 0, 0], # doc2?\n",
    "    [3, 0, 0],\n",
    "    [4, 0, 0],\n",
    "    [3, 2, 0],\n",
    "    [3, 0, 2]]\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "print(tfidf.toarray())\n",
    "transformer.idf_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}