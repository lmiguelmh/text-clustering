{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CLASSIC4 STEMMED DATABASE\n",
    "http://www.dataminingresearch.com/index.php/2010/09/classic3-classic4-datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga de la colección de textos preprocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import classicdb.fetch as fetch\n",
    "\n",
    "url = \"https://sites.google.com/site/xchgdir/public/classic4stem.tar.gz\"\n",
    "classic_home = os.path.join(fetch.get_data_home(), \"classic4stem_home\")\n",
    "corpus_root = os.path.join(classic_home, \"classic4stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading and extracting file from https://sites.google.com/site/xchgdir/public/classic4stem.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded and extracted to C:\\Users\\khas.aiur\\scikit_learn_data\\classic4stem_home\n"
     ]
    }
   ],
   "source": [
    "print(\"downloading and extracting file from \" + url)\n",
    "fetch.download_and_unzip(url, classic_home, \"classic4stem.tar.gz\")\n",
    "print(\"downloaded and extracted to \" + classic_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parseo y carga en memoria\n",
    "Sólo leeremos el vocabulario y el número de repeticiones.\n",
    "- El documento `#1552` no existe en docbyterm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docids [ 1552 , 1552 ] don't exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5896 words in vocabulary\n7094 documents readed\n7095 documents in total\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "vocabfile = os.path.join(corpus_root, \"terms.txt\")\n",
    "docwordfile = os.path.join(corpus_root, \"docbyterm.txt\")\n",
    "docfile = os.path.join(corpus_root, \"documents.txt\")\n",
    "\n",
    "with open(vocabfile, 'r') as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "bagdicts = []\n",
    "total_dicts_added = 0\n",
    "with open(docwordfile, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        w = line.strip().split()\n",
    "        if len(w) == 3:\n",
    "            docid = int(w[0]) - 1\n",
    "            wordid = int(w[1]) - 1\n",
    "            wordcount = int(w[2])\n",
    "            dicts_added = 0\n",
    "            while len(bagdicts) <= docid:\n",
    "                bagdicts.append({})  # add new dictionary\n",
    "                dicts_added += 1\n",
    "            if dicts_added > 1:\n",
    "                print(\"docids [\", docid - dicts_added + 2, \",\", docid, \"] don't exist\")\n",
    "                total_dicts_added += dicts_added - 1\n",
    "            dict = bagdicts[docid]\n",
    "            dict[vocab[wordid]] = wordcount  # add new (vocab[wordid], count) into dictionary\n",
    "            # dict[wordid] = wordcount  # add new (wordid, count) into dictionary\n",
    "            \n",
    "labels = []\n",
    "doc_labels = [\"cacm\", \"cisi\", \"cran\", \"med\"]\n",
    "with open(docfile, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        w = line.strip().split()\n",
    "        labels.append(doc_labels[int(w[1])]) \n",
    "\n",
    "print(len(vocab), \"words in vocabulary\")\n",
    "print(len(bagdicts) - total_dicts_added, \"documents readed\")\n",
    "print(len(labels), \"documents in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de tokens del corpus\n",
    "- Cada fila representa un documento y está en el orden leído del documento\n",
    "- Cada columna representa un término y ha sido reordenado (por el Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols = frequency by term, row = frequency by file\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n\nfile # 0 has 5 terms\nfile # 1 has 6 terms\nfile # 2 has 5 terms\n...\n\nSome feature terms\n['aacr', 'abandon', 'abbrevi', 'abdomen', 'abdomin', 'aberr', 'abl', 'ablat', 'abnorm', 'abolish', 'abort', 'abridg', 'abroad', 'abrupt', 'abruptli', 'abscess', 'abscissa', 'absenc', 'absent', 'absolut', 'absorb', 'absorpt', 'abstract', 'abstractor', 'abund', 'abundantli', 'academ', 'academi', 'acceler', 'accentu', 'accept', 'access', 'accessori', 'accid', 'accident', 'accommod', 'accompani', 'accomplish', 'accord', 'account', 'accredit', 'accret', 'accru', 'accumul', 'accur', 'accuraci', 'acet', 'acetazolamid', 'achiev', 'acid', 'acidosi', 'ackeret', 'acknowledg', 'acm', 'acoust', 'acquaint', 'acquir', 'acquisit', 'acromegali', 'act', 'acth', 'acti', 'actinomycin', 'action', 'activ', 'actual', 'acuiti', 'acut', 'acyclic', 'adam', 'adapt', 'add', 'addendum', 'adder', 'addit', 'address', 'adenin', 'adenocarcinoma', 'adenoma', 'adenosin', 'adenoviru', 'adequ', 'adequaci', 'adh', 'adher', 'adiabat', 'adipos', 'adjac', 'adjacen', 'adjoin', 'adjunct', 'adjust', 'adjuv', 'administ', 'administr', 'admir', 'admiss', 'admit', 'adolesc', 'adopt']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "count_matrix = vectorizer.fit_transform(bagdicts)\n",
    "print(\"cols = frequency by term, row = frequency by file\")\n",
    "print(count_matrix)\n",
    "print()\n",
    "\n",
    "for f in range(0, 3):\n",
    "    print(\"file #\", f, \"has\", int(np.sum(count_matrix[f])), \"terms\")\n",
    "print(\"...\")\n",
    "print()\n",
    "\n",
    "print(\"Some feature terms\")\n",
    "print(vectorizer.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de matriz TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFID matrix: (7095, 5896)\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(count_matrix)\n",
    "\n",
    "print(\"TFID matrix:\", tfidf.shape)\n",
    "print(tfidf.toarray())\n",
    "print()\n",
    "\n",
    "# print(\"Weights of each feature computed by TFID\")\n",
    "# print(transformer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterización y evaluación de la clusterización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document clustering with KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=4, n_init=100,\n    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n    verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTop terms per cluster:\nCluster 0: flow boundari layer pressur heat wing number bodi shock solut mach theori equat plate superson effect surfac cylinder temperatur result\nCluster 1: program comput system languag method problem time gener structur techniqu data present patient cell process paper oper function discuss code\nCluster 2: algorithm function integr matrix polynomi permut gener complex random squar invers exponenti fit solut linear gamma equat number interpol real\nCluster 3: librari inform system retriev index research scienc servic data search book docum scientif user journal base studi develop catalog literatur\nAdjusted Rand-Index: 0.368\nMutual Information based scores: 0.533\n\nHomogeneity: 0.534\nCompleteness: 0.555\nV-measure: 0.544\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.015\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "n_clusters = 4\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    init='k-means++',  # or 'random'  \n",
    "    n_init=100,  # repetition of the process to choose the best\n",
    "    max_iter=300  # iterations for kmeans\n",
    ")\n",
    "\n",
    "print(\"Document clustering with %s\" % km)\n",
    "km.fit(tfidf)\n",
    "print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(' %s' % vectorizer.get_feature_names()[ind], end='')\n",
    "    print()\n",
    "\n",
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Mutual Information based scores: %.3f\" % metrics.adjusted_mutual_info_score(labels, km.labels_))\n",
    "print()\n",
    "\n",
    "(h, c, v) = metrics.homogeneity_completeness_v_measure(labels, km.labels_)\n",
    "print(\"Homogeneity: %0.3f\" % h)\n",
    "print(\"Completeness: %0.3f\" % c)\n",
    "print(\"V-measure: %0.3f\" % v)\n",
    "print()\n",
    "\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(tfidf, km.labels_, sample_size=1000))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}