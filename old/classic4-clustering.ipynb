{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# todo we can propose a simple \"stemmer\" :)\n",
    "\n",
    "stemming_algorithm = \"<stemming algorithm>\"\n",
    "def tokenizer_porter_stemmer(text):\n",
    "    global stemming_algorithm\n",
    "    stemming_algorithm = \"PorterStemmer + SimpleTokenizer\"\n",
    "    stems = stem(tokenize(text), PorterStemmer())\n",
    "    return stems\n",
    "\n",
    "def tokenizer_snowball_stemmer(text):\n",
    "    global stemming_algorithm\n",
    "    stemming_algorithm = \"SnowballStemmer + SimpleTokenizer\"\n",
    "    stems = stem(tokenize(text), SnowballStemmer(\"english\"))\n",
    "    return stems\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def stem(tokens, stemmer):\n",
    "    # each token (word?) will be stemmed\n",
    "    # stemmed = []\n",
    "    # for item in tokens:\n",
    "    #     stemmed.append(stemmer.stem(item))\n",
    "    # return stemmed\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def filter_tokens(tokens, regex = '[a-zA-Z]'):\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search(regex, token):\n",
    "            filtered_tokens.append(token)\n",
    "            # filtered_tokens.append(re.sub(r\"[^A-Za-z]\", \"\", token))\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fetch classic database and store it in disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7095 documents\nunique labels: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from classicdb.fetch import fetch_classic\n",
    "import numpy as np\n",
    "\n",
    "dataset = fetch_classic(subset='all', categories=None, shuffle=False)\n",
    "labels = dataset.target\n",
    "unique_labels = np.unique(labels)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"unique labels: \" + str(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extracting features, PORTER stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: PorterStemmer + SimpleTokenizer\ndone in 25.241794s\nn_samples: 7095, n_features: 24489\n\nsome feature terms:\n['!', '$', '%', '&', \"'\", \"''\", \"''+p\", \"'60\", \"'68\", \"'70\", \"'70'\", \"'78\", \"'a\", \"'achondroplast\", \"'adequ\", \"'agnos\", \"'agnosic'\", \"'air\", \"'alali\", \"'alveolar\", \"'anyhow\", \"'appar\", \"'averag\", \"'azaserin\", \"'basic\", \"'bend\", \"'better\", \"'bibliograph'\", \"'bibliometr\", \"'bodi\", \"'build\", \"'built-in\", \"'campo\", \"'catch-up\", \"'cell\", \"'child\", \"'choke\", \"'citat\", \"'citeabl\", \"'cleft\", \"'cold\", \"'commando's'\", \"'commun\", \"'compar\", \"'complement\", \"'complet\", \"'comput\", \"'computer-bas\", \"'condit\", \"'core\", \"'correct\", \"'cosmopolitan\", \"'cram\", \"'crisis-st\", \"'criteria\", \"'cut-off\", \"'densiti\", \"'describ\", \"'dialysi\", \"'displac\", \"'document\", \"'documentalist\", \"'duct\", \"'earli\", \"'eclips\", \"'elementari\", \"'equival\", \"'ex\", \"'exact\", \"'exact'\", \"'failur\", \"'fat'\", \"'first\", \"'flat\", \"'flow\", \"'forc\", \"'freez\", \"'general-dynam\", \"'giant\", \"'group\", \"'growth\", \"'half\", \"'half-lif\", \"'half-life'\", \"'heat\", \"'high\", \"'higher\", \"'human\", \"'humanities'\", \"'hyperbol\", \"'immunolog\", \"'implicit\", \"'imuran\", \"'in\", \"'indirect\", \"'inevit\", \"'inform\", \"'informat\", \"'intellectu\", \"'intern\", \"'it\", \"'known-item\", \"'lamina\", \"'languag\", \"'law\", \"'level\", \"'librari\", \"'local\", \"'locat\", \"'low-beta\", \"'lower\", \"'lymphocyt\", \"'lymphocyte'\", \"'lymphocytes'\", \"'magnu\", \"'map\", \"'marca\", \"'marshal\", \"'match\", \"'mean\", \"'mesh\", \"'mix\", \"'moder\", \"'n\", \"'n'-free\", \"'neonat\", \"'new\", \"'non-visc\", \"'norm\", \"'normal\", \"'normal'\", \"'nosolog\", \"'nuclear\", \"'object\", \"'obsolesc\", \"'obsolescence'\", \"'off-step\", \"'oge\", \"'oseen\", \"'outer\", \"'oxygen-lik\", \"'perform\", \"'plane\", \"'plate\", \"'plating'\", \"'pouti\", \"'practicum\", \"'pre-scientif\", \"'primari\", \"'prime\", \"'psychiatr\", \"'psychophylaxi\", \"'psychosi\", \"'pure\", \"'purinethol\", \"'qualit\", \"'quasi-cylinder'\", \"'reader\", \"'rel\", \"'relevance'\", \"'reservoir\", \"'residu\", \"'retriev\", \"'rewarm\", \"'right\", \"'rough\", \"'s\", \"'safe\", \"'satur\", \"'schizophren\", \"'scienc\", \"'scientif\", \"'seal\", \"'second\", \"'secondari\", \"'seen\", \"'similar'\", \"'slender-bodi\", \"'slice\", \"'smooth\", \"'soft\", \"'solid\", \"'spatial\", \"'spell\", \"'spray\", \"'standard\", \"'state-of-th\", \"'steadi\", \"'stoke\", \"'straight-through\", \"'substanti\", \"'syndrom\", \"'system\", \"'tailor\", \"'term\", \"'that\", \"'the\", \"'thermal\", \"'thesaurofacet\", \"'thi\", \"'thin\", \"'third\", \"'thymus-typ\", \"'to\", \"'trace\", \"'transform\", \"'transit\", \"'tree-of-knowledg\", \"'trilamellar\", \"'tumor\", \"'tumor'\", \"'two-diagram\", \"'type\", \"'unadvisedli\", \"'use-per-item\", \"'v\", \"'v'\", \"'valid\", \"'veloc\", \"'veri\", \"'vibrat\", \"'visual\", \"'wall\", \"'what\", \"'within\", '(', ')', '*', '***medlearn***', '**-at', '*c', '*ln', '+', '++', '+-', '+-f', '+100', '+16mv', '+18degre', '+56', '+8mv', '+o', '+q', '+sin', '+size', '+u', ',', ',l', '-', '--', '-.5*ln', '-0.0904', '-0.199', '-1', '-1.087', '-1.2', '-1.23849', '-10', '-13', '-2', '-25', '-26', '-3', '-340', '-35', '-4', '-40', '-40degre', '-4degre', '-5', '-55', '-60', '-67.6', '-90', '-a', '-amid', '-an', '-bx', '-cholografin', '-construct', '-crysta-', '-crystallin', '-dash', '-distribut', '-georg', '-ii', '-impress', '-in', '-irradi', '-keto', '-ketoglutar', '-label', '-ln', '-m', '-methyl-m-trifluoromethylphenethylamin', '-multiprogram', '-n', '-o', '-p', '-plane', '-point', '-selenouracil', '-selenourea', '-semivertex-angl', '-space', '-sphingosin', '-stabil', '-stabl', '-t', '-u', '-with', '-yy', '.', '.-j', '..', '..-', '...', '..e.g.at', '..the', \"..the'solar\", './', './b/estim', './c/control-system', './d/design', '.0.', '.016', '.06', '.1.', '.11', '.2', '.2.', '.21153', '.214', '.3.', '.32', '.4', '.4.', '.45', '.465', '.5', '.5*ln', '.5.', '.6', '.64', '.766..', '.78847', '.84..', '.86', '.870..', '.9', '.92', '.93', '.95', '.97', '.and', '.but', '.ii', '.it', '.iv', '.qm', '.section', '.the', '.they', '.use', '.vi', '/', '/-dashn/', '/0.02/', '/0.04/', '/0.05/', '/0.2', '/0.2/', '/0.80', '/0.854', '/1/', '/1910/', '/2', '/2/', '/24', '/2p', '/3,300degreer/', '/3/', '/4/', '/5/', '/a/', '/abdomen/', '/abnormal/', '/about', '/absorbed/', '/aerocosmonautics/', '/aerothermochemical/', '/afresh/', '/agnos', '/all', '/altern', '/assists/', '/associ', '/at', '/attenu', '/b/', '/bacteriocin/-lik', '/blast-wave/', '/bodi', '/boom', '/boundari', '/bulk/', '/but', '/c/', '/calibr', '/canted/', '/carcinoembryonic/', '/chang', '/charg', '/choke', '/choking/', '/classical/', '/collision-free/', '/compress', '/conditioned/', '/conduct', '/contribut', '/core/', '/correlation/', '/couette-type/', '/cushion/', '/d/', '/dampometer/', '/dead', '/dead-air/', '/dead-weight/', '/decay', '/deep', '/deep/', '/deficient/', '/descript', '/destalling/', '/direct/', '/discuss', '/dividing/', '/due', '/e-d/', '/e.g.', '/effect', '/effective/', '/electr', '/energi', '/epithelial/', '/equival', '/equivalence/', '/equivalent-cone/', '/errors/', '/escape/', '/euler', '/exact/', '/except', '/exclud', '/exploding/', '/far', '/film', '/finger', '/flat', '/for', '/four', '/free-stream/', '/frozen/', '/fulli', '/fundamental/', '/generated/', '/generation/', '/given', '/hammerhead/', '/hash/', '/head/', '/heart/', '/heat', '/high', '/histori', '/hogging/', '/hub', '/hydrogen', '/hyperliptic/', '/hyperson', '/i/', '/ideal', '/ignor', '/ii/', '/iii/', '/im', '/immunity/', '/in', '/incipi', '/inclusionbody/', '/incomplet', '/incompress', '/indici', '/infinit', '/inject', '/intermedi', '/intermediate/', '/inverse/', '/inviscid', '/isotropic/', '/iv/', '/j.chem.phys.,26/2/', '/jet', '/jet-flow/', '/jmin', '/joining/', '/joint', '/k', '/k/', '/kg', '/kopal/for', '/l40/', '/lamellar', '/laminar', '/languag', '/large/', '/laval/', '/length', '/linear/', '/liver/', '/ln', '/local', '/local/', '/low', '/low/', '/lung/', '/lymphocyt', '/m', '/mach', '/macrophages/', '/magnus/', '/marker', '/max/', '/merg', '/method', '/min', '/min./cm.2', '/mirror-image/', '/mixed/', '/ml', '/model', '/modified/', '/momentari', '/most', '/n', '/n/', '/near', '/near/', '/newtonian/', '/normal/', '/nose', '/not', '/o', '/or', '/outer', '/outer/', '/over-relax/', '/palm/', '/panel', '/piston', '/plane', '/plateau/', '/polygonal/', '/poor', '/practic', '/pressur', '/pulmonari', '/quasi-steady/', '/random/', '/read', '/rebound/', '/reduc', '/relax', '/relaxed/', '/resistance/', '/restrict', '/reverse/', '/ring-wings/', '/rotational/', '/s.l.r./', '/s/', '/scale/', '/second', '/second/', '/secreted/', '/see', '/seeded/', '/self-similar/', '/signific', '/similar', '/similar/', '/slewed/', '/so', '/sonic', '/sonic-wedge/', '/space/', '/specif', '/spell', '/spillage/', '/spread/', '/stacking/', '/standardize/', '/star/', '/static/', '/stationary/', '/steadi', '/stoke', '/strong-shock/', '/strong/', '/subson', '/success', '/sum', '/superfast/', '/tailored-interface/', '/tailoring/', '/target/', '/technic', '/the', '/thi', '/thick/', '/thigh/', '/thin/', '/thu', '/tietjen', '/time/', '/total', '/trail', '/transform', '/transitional/', '/transpir', '/transtability/', '/trip', '/turbulent/', '/u1', '/uneven', '/uni-modal/', '/univer', '/urinari', '/v', '/v/', '/valid', '/veri', '/viscou', '/viscous-lay', '/w', '/wag/', '/win', '/with', '/x', '/yaw/', '0', '0,1', '0-1', '0-15', '0.', '0.000', '0.0001', '0.001', '0.002', '0.003', '0.003-', '0.004', '0.005', '0.0062', '0.00675', '0.01', '0.01-0.15', '0.01-25', '0.010', '0.011', '0.012', '0.014', '0.016', '0.019', '0.02', '0.02-in', '0.02025', '0.021', '0.0225', '0.025', '0.026', '0.029', '0.03', '0.039', '0.04', '0.0432', '0.05', '0.051', '0.056', '0.06', '0.066', '0.08', '0.088', '0.1', '0.1-1.0', '0.10', '0.11', '0.117', '0.12', '0.12.', '0.120', '0.13', '0.14', '0.1428', '0.14x10', '0.15', '0.15x106', '0.16', '0.18', '0.182', '0.1875', '0.19', '0.195e', '0.2', '0.20', '0.21', '0.211', '0.22-in', '0.24', '0.25', '0.254', '0.26', '0.262', '0.27', '0.3', '0.3-0.4', '0.3-0.6', '0.30', '0.32', '0.324', '0.33', '0.34', '0.35', '0.36', '0.367', '0.368', '0.38', '0.3x10-3', '0.4', '0.40', '0.42', '0.43', '0.44', '0.45', '0.46', '0.48', '0.5', '0.5-2.2', '0.5-3.0', '0.5.', '0.50', '0.500-in.-diamet', '0.52', '0.53', '0.55', '0.57', '0.5772', '0.58', '0.59', '0.6', '0.60', '0.62', '0.635', '0.65', '0.67-0.02', '0.69', '0.6x10', '0.7', '0.7.', '0.70', '0.715', '0.72', '0.725', '0.73', '0.737', '0.741', '0.75', '0.76-power', '0.8', '0.8-30.0', '0.80', '0.800/', '0.805', '0.823', '0.825', '0.84', '0.840', '0.85', '0.894', '0.8a', '0.9', '0.90', '0.92', '0.95', '0.952', '0.96', '0.98', '0.997', '00mm/1', '01', '029', '071182-5013', '0degre', '1', '1+i', '1+log2', '1+x', '1,000', '1,000,000', '1,023', '1,054', '1,084', '1,1-dimethyl-', '1,10-', '1,10-phenanthroline-tr', '1,100', '1,150', '1,151', '1,2', '1,2,3', '1,2-benzanthracen', '1,200', '1,200-1,600', '1,250', '1,256', '1,300', '1,328', '1,342', '1,388', '1,4000', '1,500', '1,500,000', '1,516', '1,546', '1,6-diphosph', '1,600', '1,700', '1,700f.', '1,723,568', '1,750', '1,777', '1,990', '1-', '1-1/m', '1-10', '1-13', '1-17', '1-2', '1-3', '1-4', '1-5', '1-67', '1-amino-', '1-aminocyclopentanecarboxyl', '1-bit', '1-byte', '1-dash', '1-methyl-4-nitro-5-imidazolyl', '1-month', '1-month-old', '1-per-minut', '1-percent', '1-seri', '1-t', '1-type', '1-tyrosin', '1-year', '1-year-old', '1.', '1.0', '1.00', '1.008', '1.016', '1.024', '1.03', '1.05', '1.064', '1.07/', '1.08', '1.09', '1.1', '1.10', '1.11', '1.12', '1.13', '1.13:1', '1.140', '1.15', '1.15/', '1.17', '1.2', '1.2-2.4', '1.20', '1.24', '1.25', '1.27', '1.3', '1.3.4', '1.30', '1.32', '1.33', '1.35', '1.386', '1.39', '1.3x10', '1.4', '1.40', '1.400', '1.43', '1.45', '1.450', '1.46', '1.479', '1.48', '1.5', '1.50', '1.52', '1.55', '1.58', '1.6', '1.6.', '1.60', '1.61', '1.62', '1.66', '1.67', '1.6x10', '1.7', '1.71', '1.71/', '1.72', '1.74', '1.747', '1.75', '1.76', '1.77', '1.78', '1.8', '1.8-2.0', '1.80', '1.81', '1.82', '1.85', '1.87', '1.8x10', '1.9', '1.90', '1.91', '1.92', '1.94', '1.944', '1.96', '1.97', '1.98', '1.99.1.14', '1.the', '1/', '1/100', '1/128', '1/16', '1/2', '1/2-month', '1/2-year', '1/2-year-old', '1/20', '1/3', '1/4', '1/5', '1/50', '1/6', '1/8', '1/k', '1/n', '10', '10,000', '10,000.', '10,044', '10-', '10-12', '10-15', '10-2.', '10-3', '10-30', '10-4.', '10-40', '10-7', '10-b', '10-bit', '10-earth-g', '10-fold', '10-foot', '10-percent-thick', '10-rpm', '10.', '10.0', '10.1', '10.2', '10.3']\n\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "print(\"Extracting features from the training dataset \")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             use_idf=True,\n",
    "                             max_features=None,\n",
    "                             tokenizer=tokenizer_porter_stemmer,\n",
    "                             ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(\"using: \" + stemming_algorithm)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"some feature terms:\")\n",
    "print(terms[0:1000])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extracting features, SNOWBALL stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: SnowballStemmer + SimpleTokenizer\ndone in 31.469388s\nn_samples: 7095, n_features: 24084\n\nsome feature terms:\n['!', '$', '%', '&', \"'\", \"''\", \"'+p\", \"'a\", \"'n\", \"'s\", \"'v\", '(', ')', '*', '***medlearn***', '**-at', '*c', '*ln', '+', '++', '+-', '+-f', '+100', '+16mv', '+18degre', '+56', '+8mv', '+o', '+q', '+sin', '+size', '+u', ',', ',l', '-', '--', '-.5*ln', '-0.0904', '-0.199', '-1', '-1.087', '-1.2', '-1.23849', '-10', '-13', '-2', '-25', '-26', '-3', '-340', '-35', '-4', '-40', '-40degre', '-4degre', '-5', '-55', '-60', '-67.6', '-90', '-a', '-amid', '-an', '-bx', '-cholografin', '-construct', '-crysta-', '-crystallin', '-dash', '-distribut', '-georg', '-ii', '-impress', '-in', '-irradi', '-keto', '-ketoglutar', '-label', '-ln', '-m', '-methyl-m-trifluoromethylphenethylamin', '-multiprogram', '-n', '-o', '-p', '-plane', '-point', '-selenouracil', '-selenourea', '-semivertex-angl', '-space', '-sphingosin', '-stabil', '-stabl', '-t', '-u', '-with', '-yy', '.', '.-j', '..', '..-', '...', '..e.g.at', '..the', \"..the'solar\", './', './b/estim', './c/control-system', './d/design', '.0.', '.016', '.06', '.1.', '.11', '.2', '.2.', '.21153', '.214', '.3.', '.32', '.4', '.4.', '.45', '.465', '.5', '.5*ln', '.5.', '.6', '.64', '.766..', '.78847', '.84..', '.86', '.870..', '.9', '.92', '.93', '.95', '.97', '.and', '.but', '.ii', '.it', '.iv', '.qm', '.section', '.the', '.they', '.use', '.vi', '/', '/-dashn/', '/0.02/', '/0.04/', '/0.05/', '/0.2', '/0.2/', '/0.80', '/0.854', '/1/', '/1910/', '/2', '/2/', '/24', '/2p', '/3,300degreer/', '/3/', '/4/', '/5/', '/a/', '/abdomen/', '/abnormal/', '/about', '/absorbed/', '/aerocosmonautics/', '/aerothermochemical/', '/afresh/', '/agnos', '/all', '/altern', '/assists/', '/associ', '/at', '/attenu', '/b/', '/bacteriocin/-lik', '/blast-wave/', '/bodi', '/boom', '/boundari', '/bulk/', '/but', '/c/', '/calibr', '/canted/', '/carcinoembryonic/', '/chang', '/charg', '/choke', '/choking/', '/classical/', '/collision-free/', '/compress', '/conditioned/', '/conduct', '/contribut', '/core/', '/correlation/', '/couette-type/', '/cushion/', '/d/', '/dampometer/', '/dead', '/dead-air/', '/dead-weight/', '/decay', '/deep', '/deep/', '/deficient/', '/descript', '/destalling/', '/direct/', '/discuss', '/dividing/', '/due', '/e-d/', '/e.g.', '/effect', '/effective/', '/electr', '/energi', '/epithelial/', '/equival', '/equivalence/', '/equivalent-cone/', '/errors/', '/escape/', '/euler', '/exact/', '/except', '/exclud', '/exploding/', '/far', '/film', '/finger', '/flat', '/for', '/four', '/free-stream/', '/frozen/', '/fulli', '/fundamental/', '/generated/', '/generation/', '/given', '/hammerhead/', '/hash/', '/head/', '/heart/', '/heat', '/high', '/histori', '/hogging/', '/hub', '/hydrogen', '/hyperliptic/', '/hyperson', '/i/', '/ideal', '/ignor', '/ii/', '/iii/', '/im', '/immunity/', '/in', '/incipi', '/inclusionbody/', '/incomplet', '/incompress', '/indici', '/infinit', '/inject', '/intermedi', '/intermediate/', '/inverse/', '/inviscid', '/isotropic/', '/iv/', '/j.chem.phys.,26/2/', '/jet', '/jet-flow/', '/jmin', '/joining/', '/joint', '/k', '/k/', '/kg', '/kopal/for', '/l40/', '/lamellar', '/laminar', '/languag', '/large/', '/laval/', '/length', '/linear/', '/liver/', '/ln', '/local', '/local/', '/low', '/low/', '/lung/', '/lymphocyt', '/m', '/mach', '/macrophages/', '/magnus/', '/marker', '/max/', '/merg', '/method', '/min', '/min./cm.2', '/mirror-image/', '/mixed/', '/ml', '/model', '/modified/', '/momentari', '/most', '/n', '/n/', '/near', '/near/', '/newtonian/', '/normal/', '/nose', '/not', '/o', '/or', '/outer', '/outer/', '/over-relax/', '/palm/', '/panel', '/piston', '/plane', '/plateau/', '/polygonal/', '/poor', '/practic', '/pressur', '/pulmonari', '/quasi-steady/', '/random/', '/read', '/rebound/', '/reduc', '/relax', '/relaxed/', '/resistance/', '/restrict', '/reverse/', '/ring-wings/', '/rotational/', '/s.l.r./', '/s/', '/scale/', '/second', '/second/', '/secreted/', '/see', '/seeded/', '/self-similar/', '/signific', '/similar', '/similar/', '/slewed/', '/so', '/sonic', '/sonic-wedge/', '/space/', '/specif', '/spell', '/spillage/', '/spread/', '/stacking/', '/standardize/', '/star/', '/static/', '/stationary/', '/steadi', '/stoke', '/strong-shock/', '/strong/', '/subson', '/success', '/sum', '/superfast/', '/tailored-interface/', '/tailoring/', '/target/', '/technic', '/the', '/thick/', '/thigh/', '/thin/', '/this', '/thus', '/tietjen', '/time/', '/total', '/trail', '/transform', '/transitional/', '/transpir', '/transtability/', '/trip', '/turbulent/', '/u1', '/uneven', '/uni-modal/', '/univer', '/urinari', '/v', '/v/', '/valid', '/veri', '/viscous', '/viscous-lay', '/w', '/wag/', '/win', '/with', '/x', '/yaw/', '0', '0,1', '0-1', '0-15', '0.', '0.000', '0.0001', '0.001', '0.002', '0.003', '0.003-', '0.004', '0.005', '0.0062', '0.00675', '0.01', '0.01-0.15', '0.01-25', '0.010', '0.011', '0.012', '0.014', '0.016', '0.019', '0.02', '0.02-in', '0.02025', '0.021', '0.0225', '0.025', '0.026', '0.029', '0.03', '0.039', '0.04', '0.0432', '0.05', '0.051', '0.056', '0.06', '0.066', '0.08', '0.088', '0.1', '0.1-1.0', '0.10', '0.11', '0.117', '0.12', '0.12.', '0.120', '0.13', '0.14', '0.1428', '0.14x10', '0.15', '0.15x106', '0.16', '0.18', '0.182', '0.1875', '0.19', '0.195e', '0.2', '0.20', '0.21', '0.211', '0.22-in', '0.24', '0.25', '0.254', '0.26', '0.262', '0.27', '0.3', '0.3-0.4', '0.3-0.6', '0.30', '0.32', '0.324', '0.33', '0.34', '0.35', '0.36', '0.367', '0.368', '0.38', '0.3x10-3', '0.4', '0.40', '0.42', '0.43', '0.44', '0.45', '0.46', '0.48', '0.5', '0.5-2.2', '0.5-3.0', '0.5.', '0.50', '0.500-in.-diamet', '0.52', '0.53', '0.55', '0.57', '0.5772', '0.58', '0.59', '0.6', '0.60', '0.62', '0.635', '0.65', '0.67-0.02', '0.69', '0.6x10', '0.7', '0.7.', '0.70', '0.715', '0.72', '0.725', '0.73', '0.737', '0.741', '0.75', '0.76-power', '0.8', '0.8-30.0', '0.80', '0.800/', '0.805', '0.823', '0.825', '0.84', '0.840', '0.85', '0.894', '0.8a', '0.9', '0.90', '0.92', '0.95', '0.952', '0.96', '0.98', '0.997', '00mm/1', '01', '029', '071182-5013', '0degre', '1', '1+i', '1+log2', '1+x', '1,000', '1,000,000', '1,023', '1,054', '1,084', '1,1-dimethyl-', '1,10-', '1,10-phenanthroline-tr', '1,100', '1,150', '1,151', '1,2', '1,2,3', '1,2-benzanthracen', '1,200', '1,200-1,600', '1,250', '1,256', '1,300', '1,328', '1,342', '1,388', '1,4000', '1,500', '1,500,000', '1,516', '1,546', '1,6-diphosph', '1,600', '1,700', '1,700f.', '1,723,568', '1,750', '1,777', '1,990', '1-', '1-1/m', '1-10', '1-13', '1-17', '1-2', '1-3', '1-4', '1-5', '1-67', '1-amino-', '1-aminocyclopentanecarboxyl', '1-bit', '1-byte', '1-dash', '1-methyl-4-nitro-5-imidazolyl', '1-month', '1-month-old', '1-per-minut', '1-percent', '1-seri', '1-t', '1-type', '1-tyrosin', '1-year', '1-year-old', '1.', '1.0', '1.00', '1.008', '1.016', '1.024', '1.03', '1.05', '1.064', '1.07/', '1.08', '1.09', '1.1', '1.10', '1.11', '1.12', '1.13', '1.13:1', '1.140', '1.15', '1.15/', '1.17', '1.2', '1.2-2.4', '1.20', '1.24', '1.25', '1.27', '1.3', '1.3.4', '1.30', '1.32', '1.33', '1.35', '1.386', '1.39', '1.3x10', '1.4', '1.40', '1.400', '1.43', '1.45', '1.450', '1.46', '1.479', '1.48', '1.5', '1.50', '1.52', '1.55', '1.58', '1.6', '1.6.', '1.60', '1.61', '1.62', '1.66', '1.67', '1.6x10', '1.7', '1.71', '1.71/', '1.72', '1.74', '1.747', '1.75', '1.76', '1.77', '1.78', '1.8', '1.8-2.0', '1.80', '1.81', '1.82', '1.85', '1.87', '1.8x10', '1.9', '1.90', '1.91', '1.92', '1.94', '1.944', '1.96', '1.97', '1.98', '1.99.1.14', '1.the', '1/', '1/100', '1/128', '1/16', '1/2', '1/2-month', '1/2-year', '1/2-year-old', '1/20', '1/3', '1/4', '1/5', '1/50', '1/6', '1/8', '1/k', '1/n', '10', '10,000', '10,000.', '10,044', '10-', '10-12', '10-15', '10-2.', '10-3', '10-30', '10-4.', '10-40', '10-7', '10-b', '10-bit', '10-earth-g', '10-fold', '10-foot', '10-percent-thick', '10-rpm', '10.', '10.0', '10.1', '10.2', '10.3', '10.5', '10.50', '10.8', '10.8.', '10.9', '100', '100,000', '100,000-ft', '100-137', '100-150', '100-200', '100-230', '100-bed', '100-day', '100-fold', '100.', '1000', '1000.', '100degre', '100m', '100x10', '101', '1013.', '102', '1021.', '1023', '103', '103.', '104', '104.', '1044.', '105', '1050', '1051', '106', '10642.', '1066.', '107', '1076', '108', '1081', '1086', '109', '109-placement', '10^', '10^4', '10^8', '10^i', '10^p', '10^x', '10degre', '10g', '10th', '11', '11,000', '11-', '11-20', '11-3', '11-deoxy-', '11-desoxy-17-ketosteroid', '11-inch', '11-oxy-17-ketosteroid', '11-year-old', '11.', '11.2', '11.5', '11.6', '11.60', '11.7', '11.78', '11.8', '11/45', '110', '1100', '1100/80', '1103', '1103a', '1106.', '1107', '1107.', '1108', '111', '1116.', '112', '113', '1130', '11306.', '1131.', '1135', '1139.', '114', '1142.', '1143.', '115', '116', '1161.', '1162.', '1163.', '117', '117-142', '1172.', '118', '119', '11in', '11th', '12', '12,000', '12,000..', '12,732', '12-', '12-86', '12-in', '12-inch', '12-mth-old', '12.', '12.0', '12.1', '12.4', '12.5', '12.8', '12.9', '120', '120,000', '120-inch', '1200', '12000', '1202.', '1207.', '12083.', '1209.', '121', '1211', '1218', '122', '123', '124', '124.9', '125', '125,000', '125-word', '1250', '126', '127', '128', '1281.', '1287.', '1288.', '129', '12degre', '12k', '13', '13,000', '13,676', '13-6', '13-fold', '13-year-old', '13.', '13.1', '13.5', '13.6', '13.8', '13.9', '13/4', '130', '130,000', '1300', '1300-', '1303.', '131', '131-sodium', '131-tag', '1311', '1319.', '132', '132,808', '132-bit', '132.', '1320.', '133', '133.', '134', '135', '135,938', '1350', '136', '1361.', '1362.', '1368.', '137', '1375.', '1377.', '1378.', '1379.', '138', '1384.', '1388.', '13891', '139', '14', '14,000', '14,592', '14,938', '14-19', '14-day', '14-in', '14-year-', '14.1', '14.3', '14.4', '14.7', '14.8', '140', '140,000', '1400']\n\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Extracting features from the training dataset\")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             use_idf=True,\n",
    "                             max_features=None,\n",
    "                             tokenizer=tokenizer_snowball_stemmer,\n",
    "                             ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(\"using: \" + stemming_algorithm)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"some feature terms:\")\n",
    "print(terms[0:1000])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n        init_size=1000, max_iter=100, max_no_improvement=10, n_clusters=4,\n        n_init=1, random_state=None, reassignment_ratio=0.01, tol=0.0,\n        verbose=False)\ndone in 0.153s\n\ntrue labels vs cluster labels\n[3 0 0 1 2 0 0 1 0 3 0 0 2 0 0 1 3 2 1 2 2 0 3 0 0 3 3 0 0 0 3 0 3 0 3 0 3\n 3 1 0 2 3 0 3 0 0 0 2 2 0]\n[1 3 0 3 1 0 2 3 3 0 2 2 1 3 0 3 1 1 3 1 1 2 1 0 2 1 1 0 3 0 0 3 1 0 1 3 1\n 1 3 0 1 1 2 1 3 2 2 1 1 2]\n\nHomogeneity: 0.509\nCompleteness: 0.488\nV-measure: 0.498\nAdjusted Rand-Index: 0.359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.013\n\nTop terms per cluster:\nCluster 0: comput . program method fortran algol , use techniqu algorithm\nCluster 1: . , flow pressur boundari number effect layer equat method\nCluster 2: algorithm ) ( ] [ integr function $ matrix polynomi\nCluster 3: , . inform librari program use languag comput data retriev\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                     init_size=1000, batch_size=1000, verbose=False)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"true labels vs cluster labels\")\n",
    "print(labels[0:50])\n",
    "print(km.labels_[0:50])\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}