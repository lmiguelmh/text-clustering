{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HCI\n",
    "Execute the first cell that will download and load the corpus into memory. Or follow the next steps.\n",
    "1. Replace the `corpus_root` with the directory where the course files are\n",
    "2. Every file should have the next form: week-the_file.pdf.txt\n",
    "3. You can use Glyph&Cog pdftotext.exe (http://www.foolabs.com/xpdf/download.html) or another tool to convert the pdfs to plain text:\n",
    "\n",
    "```\n",
    "@echo off\n",
    "FOR %%F IN (*.pdf) DO (\n",
    "echo working in %%F\n",
    "pdftotext.exe -layout \"%%F\" \"%%F.layout.txt\"\n",
    "rem pdftotext.exe -raw \"%%F\" \"%%F.raw.txt\"\n",
    "rem pdftotext.exe -table \"%%F\" \"%%F.table.txt\"\n",
    ")\n",
    "PAUSE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download and extracting file from https://sites.google.com/site/xchgdir/public/hci.tar.gz?attredirects=0&d=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded and extracted to C:\\Users\\khas.aiur\\scikit_learn_data\\hci_home\n['w1', 'w10', 'w11', 'w12', 'w13', 'w14', 'w15', 'w16', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import classicdb.fetch as fetch\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "\n",
    "url = \"https://sites.google.com/site/xchgdir/public/hci.tar.gz?attredirects=0&d=1\"\n",
    "data_home = fetch.get_data_home()\n",
    "classic_home = os.path.join(data_home, \"hci_home\")\n",
    "print(\"download and extracting file from \" + url)\n",
    "fetch.download_and_unzip(url, classic_home, \"hci.tar.gz\")\n",
    "print(\"downloaded and extracted to \" + classic_home)\n",
    "corpus_root = os.path.join(classic_home, \"hci\")\n",
    "# corpus_root = \"E:\\\\data\\\\sesiones\"\n",
    "corpus_reader = CategorizedPlaintextCorpusReader(corpus_root, r'(w[0-9]+)\\-.*.txt', cat_pattern=r'(\\w+)/*', encoding='ISO-8859-1')\n",
    "\n",
    "cats = corpus_reader.categories()\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance and similarity for a given term in the complete non-stemmed corpus.\n",
    "** área != area != areas != áreas **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCORDANCE:  LLAMATIVO\nDisplaying 1 of 1 matches:\na productividad · la eficiencia · la usabilidad · que el producto sea llamativo 18 la naturaleza de la interacción persona - computador : la interacc\n\nSIMILARITY:  LLAMATIVO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n"
     ]
    }
   ],
   "source": [
    "terms = [\"llamativo\"]\n",
    "\n",
    "# http://www.nltk.org/book/ch01.html\n",
    "from nltk.text import Text  \n",
    "all_text = Text(word.lower() for word in corpus_reader.words())\n",
    "\n",
    "for term in terms:\n",
    "    print(\"CONCORDANCE: \", term.upper())\n",
    "    all_text.concordance(term, width=150)\n",
    "    print()\n",
    "    \n",
    "    print(\"SIMILARITY: \", term.upper())\n",
    "    all_text.similar(term)\n",
    "    print()\n",
    "\n",
    "# print(\"COMMON CONTEXTS: \")\n",
    "# all_text.common_contexts([term, \"teorías\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCORDANCE:  HOFSTEDE\n*  w16-Internacionalización.pdf.layout.txt\nDisplaying 4 of 4 matches:\n teoría de las dimensiones culturales · geert hofstede desarrolló un modelo sistemático para evaluar\ns uai perú chile extraído de http :// geert - hofstede . com / countries . html ( datos al año 2010 \n teoría de las dimensiones culturales · geert hofstede establece que estos resultados son relativos \n teoría de las dimensiones culturales · geert hofstede establece que estos resultados son relativos \n\n"
     ]
    }
   ],
   "source": [
    "terms = [\"hofstede\"]\n",
    "\n",
    "from nltk.text import Text\n",
    "from nltk.text import ConcordanceIndex\n",
    "from nltk.text import ContextIndex\n",
    "\n",
    "filetexts = {}\n",
    "fileconcs = {}\n",
    "fileconts = {}\n",
    "for fileid in corpus_reader.fileids():\n",
    "    filetexts[fileid] = Text(word.lower() for word in corpus_reader.words(fileid))\n",
    "    fileconcs[fileid] = ConcordanceIndex(word.lower() for word in corpus_reader.words(fileid))\n",
    "    # fileconts[fileid] = ContextIndex(word.lower() for word in corpus_reader.words(fileid))\n",
    "\n",
    "for term in terms:\n",
    "    print(\"CONCORDANCE: \", term.upper())\n",
    "    for fileid in corpus_reader.fileids():\n",
    "        if fileconcs[fileid].offsets(term):\n",
    "            print(\"* \", fileid)\n",
    "            filetexts[fileid].concordance(term, width=100)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance and similarity for a given term in the complete stemmed corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCORDANCE\nDisplaying 25 of 45 matches:\narroll de tecnic multiprocessing desarroll de ambient windows ajust del tiemp de respuest a las caracterist human 14 raic histor ( iv ) factor human \nil model predict ( analisis de accion ) : analisis cuantit par estim el tiemp necesari par realiz las tar , en bas a estim de tiemp par accion tipic \nntit par estim el tiemp necesari par realiz las tar , en bas a estim de tiemp par accion tipic ( inspeccion orient a la eficient ) 9 prueb de usabil \nusuari · aplic en etap tempran del desarroll desventaj : · requier much tiemp · requier prepar ( experient ) previ · requier metodolog de definicion \ndispon se ejecut 22 model predict el model keystrok level : propuest de tiemp tipic , com : presion una tecl : 0,2 ­ 1,2 sec . ( promedi 0.35 ) posic\ndit : evalu mas de un aspect recolect dat primari ­ mediacion cuantit ( tiemp , numer de error etc. ) comp altern de diseñ , evalu requer de eficient\nem de usabil . 1 problem cosmet : el problem ser arregl a men que exist tiemp extra dispon en el proyect . 2 problem menor : arregl el problem es de \nm sobr lo que esta ocurr , a traves de una apropi retroaliment dentr de tiemp razon . n2 . coincident entre el sistem y el mund real : el sistem deb \nuari son capac de complet con exit tar especif . permit identific cuant tiemp le tom a los usuari complet tar especif . permit determin el grad de sa\nif de las tar establec . el porcentaj de exit y fracas de cad tare . el tiemp emple por el usuari par cad tare . analisis de los exit y fracas . ¿por\n y fracas . ¿por que el usuari fracas ? ¿qu deb mejor ? analisis de los tiemp . ¿es adecu el tiemp emple por el usuari ? ¿por que no ? ¿qu problem se\nel usuari fracas ? ¿qu deb mejor ? analisis de los tiemp . ¿es adecu el tiemp emple por el usuari ? ¿por que no ? ¿qu problem se confirm ? ¿qu proble\n·bmw - argentin por ejempl la web argentin de bmw no utiliz correct los tiemp verbal ni el ton tipic del pais , sin el español castellan : manteng el\nellan : manteng el control ... , manteng segur ... ·movist ­ argentin ­ tiemp verbal ·speedy ­ argentin ­ tiemp verbal ·mcdonald 's ­ itali individua\nanteng segur ... ·movist ­ argentin ­ tiemp verbal ·speedy ­ argentin ­ tiemp verbal ·mcdonald 's ­ itali individual ­ baj context ·mcdonald 's ­ chi\n el usuari recib con frecuenci mas inform de la que pued proces al mism tiemp ­ ejempl : pagin de inici de un portal de internet · la atencion funcio\nur ni la mid con precision 37 canal i/o : reaccion respuest a estimul = tiemp de reaccion + tiemp de movimient tiemp de reaccion ­ depend del tip de \ncision 37 canal i/o : reaccion respuest a estimul = tiemp de reaccion + tiemp de movimient tiemp de reaccion ­ depend del tip de estimul : visual ­ 2\n : reaccion respuest a estimul = tiemp de reaccion + tiemp de movimient tiemp de reaccion ­ depend del tip de estimul : visual ­ 200 ms audit ­ 150 m\npend del tip de estimul : visual ­ 200 ms audit ­ 150 ms dolor ­ 700 ms tiemp de movimient ­ depend de edad , habil , entren etc. 38 sent cinestes es\ns las emocion 41 el olfat adapt si los receptor son expuest durant much tiemp a un mism olor pierd select la sensibil a ese olor gran variacion indiv\nnu y muy rap los proces encarg de analiz en la memori de trabaj necesit tiemp par realiz su funcion y pued ocurr que la pierd antes de almacen por es\nensorial tien asoci memori dond la inform se almacen por cort period de tiemp ( milesim de segund ) la funcion de estas memori es reten la inform par\nn trabaj se mantien en ella mientr que los estem usand y prest atencion tiemp de acces : 70 ms 56 funcion de la memori oper retencion de inform sopor\nc en las convers cotidian ejempl : ahorr , gast , desaprovech ( diner ) tiemp defend , atac , retir ( concept belic ) ide ( abstract ) naveg , explor\n\nSIMILARITY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mensaj trat nivel numer hotel manej result cumplimient equip proces\ncompar analisis lenguaj principi estil punt diseñ uso tip list\n\nCOMMON CONTEXTS\n('The following word(s) were not found:', 'tiemp teorías')\n"
     ]
    }
   ],
   "source": [
    "term = \"tiempos\"\n",
    "\n",
    "# http://www.nltk.org/book/ch01.html\n",
    "from nltk.text import Text\n",
    "from nltk import PorterStemmer, LancasterStemmer, word_tokenize, SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "term = stemmer.stem(term)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "all_text = Text(stemmer.stem(word.lower()) for word in word_tokenize(corpus_reader.raw(), 'spanish'))\n",
    "\n",
    "print(\"CONCORDANCE\")\n",
    "all_text.concordance(term, width=150)\n",
    "print()\n",
    "\n",
    "print(\"SIMILARITY\")\n",
    "all_text.similar(term)\n",
    "print()\n",
    "\n",
    "print(\"COMMON CONTEXTS\")\n",
    "all_text.common_contexts([term, \"teorías\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}